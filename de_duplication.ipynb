{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7tyN6sWTKGD",
        "outputId": "e1a01ff2-e79c-49e2-ee01-ea15f9d969f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasketch\n",
            "  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.13.1)\n",
            "Downloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasketch\n",
            "Successfully installed datasketch-1.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install datasketch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datasketch import MinHash\n",
        "import re\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz-ZIE8MLzNa"
      },
      "source": [
        "#MinHash Jaccard similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRpxvzYlS3aY",
        "outputId": "d2a72123-137b-484d-b034-89f12dbf6807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "# Function to read the file and break it into sentences\n",
        "def read_punjabi_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    sentences = re.split(r'[।?!]|।।', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences\n",
        "\n",
        "# Create DataFrame from sentences\n",
        "def create_punjabi_df(sentences):\n",
        "    df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "    df[\"hash\"] = \"NA\"  \n",
        "    return df\n",
        "\n",
        "# MinHash calculation and deduplication\n",
        "def calculate_minhash_and_deduplicate(df, threshold=0.2):\n",
        "    print(f\"Length of DataFrame before deduplication: {len(df)}\")\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        m1 = MinHash()\n",
        "        for token in df.iloc[i, 0].split():  \n",
        "            m1.update(token.encode('utf-8'))\n",
        "        df.at[i, \"hash\"] = m1\n",
        "\n",
        "    i = 0\n",
        "    while i < len(df):\n",
        "        index_list = []\n",
        "        for j in range(i + 1, len(df)):\n",
        "            if df.iloc[i, 1].jaccard(df.iloc[j, 1]) >= threshold:\n",
        "                index_list.append(j)\n",
        "\n",
        "        df = df.drop(index_list).reset_index(drop=True)\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Length of DataFrame after deduplication: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "file_path = \"Timepass.txt\" \n",
        "punjabi_sentences = read_punjabi_text_file(file_path)\n",
        "\n",
        "df = create_punjabi_df(punjabi_sentences)\n",
        "df = calculate_minhash_and_deduplicate(df)\n",
        "\n",
        "print(f\"Number of unique sentences: {len(df)}\")\n",
        "\n",
        "output_file_path = file_path.replace(\".txt\", \"_de_duplicated.txt\")\n",
        "df.to_csv(output_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Deduplicated file saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYkltMFCL4CL"
      },
      "source": [
        "#Deduplicate sentences using cosine similarity with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7dpSpvWf6Dz",
        "outputId": "503fb701-611a-4620-db7c-0f94766256eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            sentence hash\n",
            "0          ਸੀ ਦਸਮ ਗ੍ਰੰਥ ਸਾਹਿਬ ਜੀ ਦੇ ਪੰਨਾ ਨੰਬਰ ੧੫੫ ਤਕ   NA\n",
            "1          ਸੀ ਦਸਮ ਗ੍ਰੰਥ ਸਾਹਿਬ ਜੀ ਦੇ ਪੰਨਾ ਨੰਬਰ ੧੫੫ ਤਕ   NA\n",
            "2          ਸੀ ਦਸਮ ਗ੍ਰੰਥ ਸਾਹਿਬ ਜੀ ਦੇ ਪੰਨਾ ਨੰਬਰ ੧੫੫ ਤਕ   NA\n",
            "3  ਕੂ ਚਿਹਨ ਅਰੁ ਬਰਨ ਜਾਤਿ ਅਰੁ ਪਾਤਿ ਨਹਿਨ ਜਿਹ ॥ \\nਰੰਗ...   NA\n",
            "Cosine Similarity Deduplication:\n",
            "                                            sentence\n",
            "0          ਸੀ ਦਸਮ ਗ੍ਰੰਥ ਸਾਹਿਬ ਜੀ ਦੇ ਪੰਨਾ ਨੰਬਰ ੧੫੫ ਤਕ\n",
            "1  ਕੂ ਚਿਹਨ ਅਰੁ ਬਰਨ ਜਾਤਿ ਅਰੁ ਪਾਤਿ ਨਹਿਨ ਜਿਹ ॥ \\nਰੰਗ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def cosine_similarity_deduplication(sentences, threshold=0.8):\n",
        "    \"\"\"Deduplicate sentences using cosine similarity with TF-IDF.\"\"\"\n",
        "    df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(df[\"sentence\"])\n",
        "\n",
        "    to_remove = set()\n",
        "\n",
        "    for i in range(tfidf_matrix.shape[0]):\n",
        "        if i in to_remove:\n",
        "            continue\n",
        "        cosine_similarities = cosine_similarity(tfidf_matrix[i], tfidf_matrix).flatten()\n",
        "        for j in range(i + 1, len(cosine_similarities)):\n",
        "            if cosine_similarities[j] >= threshold:\n",
        "                to_remove.add(j)\n",
        "\n",
        "    df = df.drop(index=to_remove).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# punjabi_sentences = [\"ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਦੂਜਾ ਵਾਕ ਹੈ।\"]\n",
        "print(df)\n",
        "df_cosine = cosine_similarity_deduplication(punjabi_sentences)\n",
        "print(\"Cosine Similarity Deduplication:\")\n",
        "print(f\"Number of unique sentences: {len(df_cosine)}\")\n",
        "\n",
        "output_file_path = file_path.replace(\".txt\", \"_de_duplicated.txt\")\n",
        "df_cosine.to_csv(output_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Deduplicated file saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpRqj50eMBw1"
      },
      "source": [
        "#Deduplicate sentences using Levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9SRJNfchqeJ",
        "outputId": "6eee126d-bfa2-4399-c819-aa8696bb7387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.26.0 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_Levenshtein-0.26.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.26.0 python-Levenshtein-0.26.0 rapidfuzz-3.9.7\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IkS25ruhld3",
        "outputId": "9692c456-4845-41f0-dafa-2d649f4bb979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Levenshtein Distance Deduplication:\n",
            "             sentence\n",
            "0  ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\n",
            "1     ਇਹ ਦੂਜਾ ਵਾਕ ਹੈ।\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def levenshtein_deduplication(sentences, max_distance=2):\n",
        "    \"\"\"Deduplicate sentences using Levenshtein distance.\"\"\"\n",
        "    df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "    to_remove = set()\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if i in to_remove:\n",
        "            continue\n",
        "        for j in range(i + 1, len(df)):\n",
        "            if levenshtein_distance(df.iloc[i][\"sentence\"], df.iloc[j][\"sentence\"]) <= max_distance:\n",
        "                to_remove.add(j)\n",
        "\n",
        "    df = df.drop(index=to_remove).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# punjabi_sentences = [\"ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਇਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਦੂਜਾ ਵਾਕ ਹੈ।\"]\n",
        "# df_levenshtein = levenshtein_deduplication(punjabi_sentences)\n",
        "print(\"Levenshtein Distance Deduplication:\")\n",
        "\n",
        "print(f\"Number of unique sentences: {len(df_levenshtein)}\")\n",
        "\n",
        "output_file_path = file_path.replace(\".txt\", \"_de_duplicated.txt\")\n",
        "df_levenshtein.to_csv(output_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Deduplicated file saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0niqzOeNMX2Y"
      },
      "source": [
        "#Deduplicate sentences using fingerprinting (hashing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR8FFtSGi7Ki",
        "outputId": "ee1dec9c-7aa0-4345-a213-a47aa62e9406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXUyQ2voE7jB",
        "outputId": "29f097f0-3755-4ad3-e041-fe881f7213b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fingerprinting Deduplication:\n",
            "                                            sentence\n",
            "0          ਸੀ ਦਸਮ ਗ੍ਰੰਥ ਸਾਹਿਬ ਜੀ ਦੇ ਪੰਨਾ ਨੰਬਰ ੧੫੫ ਤਕ\n",
            "1  ਕੂ ਚਿਹਨ ਅਰੁ ਬਰਨ ਜਾਤਿ ਅਰੁ ਪਾਤਿ ਨਹਿਨ ਜਿਹ ॥ \\nਰੰਗ...\n"
          ]
        }
      ],
      "source": [
        "def fingerprint_deduplication(sentences):\n",
        "    \"\"\"Deduplicate sentences using fingerprinting (hashing).\"\"\"\n",
        "    df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "    df[\"hash\"] = df[\"sentence\"].apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n",
        "    df.drop_duplicates(subset=\"hash\", inplace=True)\n",
        "    df.drop(columns=[\"hash\"], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# punjabi_sentences = [\"ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਇੱਕ ਸਜਗ ਵਾਕ ਹੈ।\", \"ਇਹ ਦੂਜਾ ਵਾਕ ਹੈ।\"]\n",
        "df_fingerprint = fingerprint_deduplication(punjabi_sentences)\n",
        "print(\"Fingerprinting Deduplication:\")\n",
        "print(f\"Number of unique sentences: {len(df_fingerprint)}\")\n",
        "\n",
        "output_file_path = file_path.replace(\".txt\", \"_de_duplicated.txt\")\n",
        "df_fingerprint.to_csv(output_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"Deduplicated file saved to {output_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
